---
layout: post
title: Practical Comparison Between DeepSpeed FastGen and vLLM
date: 2024-03-06 11:59:00-0800
comments: true
categories: nlp


---


# Practical Comparison Between DeepSpeed FastGen and vLLM

In recent blog posts, both DeepSpeed and vLLM teams have kind of claim that they are better than each other. 
Also discussion on [Reddit](https://www.reddit.com/r/LocalLLaMA/comments/187jttj/vllm_vs_deepspeed_contradictory_reports/):
> The DeepSpeed team recently published a blog post stating that their inference time is 2.4 times faster than vLLM on a 4xA100 setup. Subsequently, the vLLM team countered with their own blog post, asserting that their experiments on a single A100 demonstrate faster performance than DeepSpeed. What are your thoughts on this? Does anyone have their own benchmarks to share?

Thus, as users, we better evaluate it ourselves before really using either one of them in a large scale. 
Both speed and memory are my consideration and the GPU resource applicable to myself is A100-40GB.

Thus, I will be testing the **speed** and **memory** on my own practical experiments, where my use case contains at most `1500` tokens as inputs and maximum `500` tokens as output. 

> You should test on your own if you have different use cases. 

## Configuration
```
model_size: 7B and 22B
Number of test instance: 100
transformers==
D
```